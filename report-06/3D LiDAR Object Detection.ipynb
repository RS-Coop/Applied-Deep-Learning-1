{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will begin by summarizing the following paper: [An LSTM Approach to Temporal 3D Object Detection in LiDAR Point Clouds](https://arxiv.org/abs/2007.12392). I will also try to inject my own thoughts about what certain details in this paper might mean for my own project. I will then focus on a few key aspects of this paper that will be vital in moving forward with my own project independent of its exact goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light Detection and Ranging (LiDAR) is a type of 3 dimensional sensor that has become more and more ubiquitous in recent years. The authors point out that this type of sensor is a boon for autonomous and robotic systems. The goal for computer vision using LiDAR is semantic segmentation of a scene, or frame given most LiDAR sensors are used continuously. Noting that most methods for achieving this goal only use a single frame as inputs the authors propose to use recurrent networks that incorporate past frames into the semantic segmentation process. This is further motivated by the fact that large multi-frame datasets have recently become available. Namely, the nuScenes and Waymo Open datasets each contain a thousand sequences of LiDAR data -- the latter of the two is used in the paper. I will note that, for my project, it may be a good idea to investigate the merits of both datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There has been a host of work in the field of 3D object detection using deep neural networks. Some apporach this task with 2D image processing techniques, while others have looked directly at the 3D data. Notable for this work are [VoxelNet](https://arxiv.org/abs/1711.06396) and [SparseConv](https://arxiv.org/abs/1706.01307). The former voxelizes 3D point cloud data to turn it into a more usable regular grid. The latter restricts 3D convolutions to regions of activity, significantly increasing efficiency as 3D point cloud data is highly sparse. A few of the previous works mentioned try to include the temporal dimension, but the authors note that their apporach is novel and distinct in its recurrent nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architechture the authors propose consists of three main components: feature extractor, recurrent module, and object identifier. In the feature extractor a frame of 3D point cloud data is passed to a sparse CNN after being voxelized. The original 3D data is in the form of many xyz points, and the voxelization is sparse. The sparse CNN is a U-Net style network that uses sparse 3D convolutions and 3D max pooling. As in the 2D version, there are skip connections between the downsampling and upsampling sides of the network. Before moving to the recurrent module, the extracted features are de-voxelized back into point cloud form. The authors cite [DOPS](https://arxiv.org/abs/2004.01170) as their reference for the feature extraction. \n",
    "\n",
    "Once the output of this initial section has been obtained (which we note is of the same shape as the input), it is passed to an LSTM module for fusion with the stored state. This module is mostly the same as the standard, but with two key differences. Specifically, the fully connected layer inside the module is replaced with another (smaller) sparse U-Net style CNN. This allows for the features from the input and previous timesteps to be easily fused and processed. The second difference is that the full hidden state and cell memory aren't kept intact, but instead are sampled for the points with high \"semantic scores\". The authors note that this is \"obtained from the pre-trained single frame detection model\", but that seems to be an unsatisfactory description. Furthermore, the stored state is transformed from the coordinate frame of the last input to the coordinate from of the current input. The authors refer to this as an \"Ego Motion Transformation\". The LSTM input, hidden state, and cell memory are concatenated along the feature dimension and then jointly voxelized to account for the spatial changes due to the motion of the scene. After passing through the internals of the LSTM module the stored state (but not the output) is then de-voxelized. I note here that this seems to be a lot of back and forth between the voxelized form and the point cloud form. It doesn't seem to hold the researchers back from achieving fast inferences, but I wonder if there is a simpler way to deal with this. One final process is used on the stored state to further account for the scene motion.\n",
    "\n",
    "The last step is to preform the actual object detection on the feature data from the LSTM module. Within this there are actual three sub-processes that occur. First, per voxel bounding boxes are generated using three layers of sparse convolutions for each of center, rotation, height, length, and width. This is then de-voxelized with each point inside a voxel assuming that voxels bounding box. Second, a graph is constructed from groups of points that share similar predicted object centers, and a graph based convolution is performed. In inference the final step is applying non-maximum suppression to obtain final 3D semantic segmentation results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training the network is initially using single frame input. The loss for the output is a hybrid of regression and classification. Given that each bounding box is actually described with five different parameters, the authors opt for an \"integrated box corner loss\" (on the regression side) which can update all the attributes at once. For the classification loss the authors use a 70% Intersection Over Union (IOU) with the ground truth as a positive prediciton and the rest as negative. The integrated box corner loss uses all of the attributes described above to to compute the eight predicted corner locations. A per point regression loss is then applied which will automatically propogate back to the original attributes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In testing four LiDAR frames are used as input, and the performance metric being used is mean average precision (mAP). The authors find that their model performs very well. Specifically, they achieve 63.6% mAP which is a 7.5% increase over a similar network using one frame without the LSTM and a  1.2% increase over a network that uses four frame concatenation and no LSTM. Qualitatively, it can be seen that the network produces better bounding boxes and has fewer false positives. The authors also show that adding more frames to the input increases the accuracy of the model -- with a single frame version still achieving 58.7% mAP. Beyond dataset performance the authors succeed in two other important areas: memory and computational efficiency. By reducing the number of points that are being operated on, and becuase the LSTM adds relatively little in computational cost, the model is smaller and can run inference in 19ms, well under the 10hz LiDAR data rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper was well written, easy to follow, and concise in its contributions and achievements. However, there were two important aspects that I believe could have been further explained. First, the authors mention that they sub-sampled the LiDAR point cloud data but are quite vague and inconsistent on this. They claim that the backbone (the input U-Net) features are sub-sampled, but earlier they had claimed that only the LSTM stored state is sub-sampled from the previous computation. Second, the authors discuss using multiple frames as input, but imply that there method does not use concatenation. I am confused then how four frames are used more efficiently. Perhaps they are using four single frame passses to object detect in one future scene, but that seems to be avoiding the point of using a recurrent layer. More investigation is needed for me to fully understand these two aspects, but I feel that my learning in class and on the reports has prepared me well and allowed me to understand the core of this paper. There are a few topics that I am uncertain about due to a lack of exposure which will be discussed in the Key Components section below. \n",
    "\n",
    "In terms of how this paper informs my own project I see a number of areas that could be improved or explored:\n",
    "- **Object Detection Head:** I see plenty of opportunity for exploration in the last portion of the network. The method they are using seems (at least to me currently) somewhat specific, and I wonder if more thought can be put into a deliberate approach. As well, I have read about some problems with NMS, so there might be other options there.\n",
    "\n",
    "\n",
    "- **Voxelization:** Within the network there is a decent amount of voxelization (point cloud to voxel grid) and de-voxelization (vice versa). I wonder if there is a way to maintain the voxel encoding throughout and only convert back to the point cloud at the very end.\n",
    "\n",
    "\n",
    "- **Recurrent Layer:** The authors have chosen a standard LSTM (with the sparse CNN change) as their recurrent layer. Perhaps a GRU would allow for quicker training and a smaller model, but maybe it would cost too much accuracy. There might be some other variant that deserves investigation, and I think this is a good place for alteration.\n",
    "\n",
    "\n",
    "- **Backbone Network:** A sparse U-Net style network is being used as the backbone for processing the 3D data. We have discussed in class other such networks that may perform better such as the DeepLabv3+.\n",
    "\n",
    "\n",
    "- **Scene Flow:** Somewhat independent of the model specifics, predicting scen flow (i.e. the movement of objects) is also desirable and is mentioned by the authors as future work. Furthermore, this may allow for better computation and transformation of the recurrent state.\n",
    "\n",
    "\n",
    "- **Model Reduction:** As it stands the model is quite complicated with a fair number of layers and techincal components. I believe some of this could be pared down etheir with existing reduction techniques (e.g. pruning), or by changing the model architechture. The recurrent layer changes and voxelization discussed above also fit into this idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the following topics is a key aspect of building the model described in the paper and achieving their results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waymo Open Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset from the Waymo self-driving company has 1000 diverse sequences. There are about 200 frames in each sequnce with a frame rate of 100ms. In addition to LiDAR data there is also 2D camera data. Within each LiDAR scene there are labels and bounding boxes for vehicles, pedestrians, cyclists, and signs. Each scene also has a global coordinate frame and a vehicle coordinate frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire dataset is 2TB which is ... scary to say the least. It is broken into 32 training chunks and 8 validation chunks of about 25GB. I have downloaded one of the chunks (which can be done [here](https://waymo.com/open/download/)), and then placed one of the files in the *waymo-single* folder.\n",
    "\n",
    "Below we have loaded the data and examined some of its details. We note that we did not visualize the 3D point cloud as Waymo does not have a publicly available system for this, and I did not have time to figure something else out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from waymo_open_dataset.utils import range_image_utils\n",
    "from waymo_open_dataset.utils import transform_utils\n",
    "from waymo_open_dataset.utils import  frame_utils\n",
    "from waymo_open_dataset import dataset_pb2 as open_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waymo provides a GitHub repository that contains a number of useful tools for working with their data and using it in deep learning. This package has been imported above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'waymo-single/segment-1005081002024129653_5313_150_5333_150_with_camera_labels.tfrecord'\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(path, compression_type='')\n",
    "for data in dataset:\n",
    "    frame = open_dataset.Frame()\n",
    "    frame.ParseFromString(bytearray(data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(range_images, camera_projections,\n",
    " range_image_top_pose) = frame_utils.parse_range_image_and_camera_projection(\n",
    "    frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"1005081002024129653_5313_150_5333_150\"\n",
      "camera_calibrations {\n",
      "  name: FRONT\n",
      "  intrinsic: 2083.091212133254\n",
      "  intrinsic: 2083.091212133254\n",
      "  intrinsic: 957.2938286685071\n",
      "  intrinsic: 650.5697927719348\n",
      "  intrinsic: 0.04067236637270731\n",
      "  intrinsic: -0.3374271466716414\n",
      "  intrinsic: 0.0016273829099200004\n",
      "  intrinsic: -0.0007879327563938157\n",
      "  intrinsic: 0.0\n",
      "  extrinsic {\n",
      "    transform: 0.9999151800844592\n",
      "    transform: -0.008280529275085654\n",
      "    transform: -0.010053132426658727\n",
      "    transform: 1.5444145042510942\n",
      "    transform: 0.008380895965622895\n",
      "    transform: 0.9999150476776223\n",
      "    transform: 0.009982885888937929\n",
      "    transform: -0.022877347388980857\n",
      "    transform: 0.009969614810858722\n",
      "    transform: -0.010066293398396434\n",
      "    transform: 0.9998996332221252\n",
      "    transform: 2.115953541712884\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 1.0\n",
      "  }\n",
      "  width: 1920\n",
      "  height: 1280\n",
      "  rolling_shutter_direction: RIGHT_TO_LEFT\n",
      "}\n",
      "camera_calibrations {\n",
      "  name: FRONT_LEFT\n",
      "  intrinsic: 2083.7318205002007\n",
      "  intrinsic: 2083.7318205002007\n",
      "  intrinsic: 970.3176094982554\n",
      "  intrinsic: 632.5251399459935\n",
      "  intrinsic: 0.0455796545917619\n",
      "  intrinsic: -0.35403417476148374\n",
      "  intrinsic: 0.0006196783672492779\n",
      "  intrinsic: 0.0014815820487813483\n",
      "  intrinsic: 0.0\n",
      "  extrinsic {\n",
      "    transform: 0.6999081394293895\n",
      "    transform: -0.7142196095619765\n",
      "    transform: -0.004352663279896696\n",
      "    transform: 1.4966436414481195\n",
      "    transform: 0.7140873020002204\n",
      "    transform: 0.6998755279288658\n",
      "    transform: -0.015923898025841435\n",
      "    transform: 0.09550840486978261\n",
      "    transform: 0.014419482741635486\n",
      "    transform: 0.008037084261673145\n",
      "    transform: 0.9998637326126167\n",
      "    transform: 2.116104849961399\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 1.0\n",
      "  }\n",
      "  width: 1920\n",
      "  height: 1280\n",
      "  rolling_shutter_direction: RIGHT_TO_LEFT\n",
      "}\n",
      "camera_calibrations {\n",
      "  name: FRONT_RIGHT\n",
      "  intrinsic: 2082.2327597604753\n",
      "  intrinsic: 2082.2327597604753\n",
      "  intrinsic: 955.4046240156483\n",
      "  intrinsic: 653.698903155303\n",
      "  intrinsic: 0.044328526985070366\n",
      "  intrinsic: -0.3399662257128328\n",
      "  intrinsic: 0.0016037200659441385\n",
      "  intrinsic: -0.0014698906603900644\n",
      "  intrinsic: 0.0\n",
      "  extrinsic {\n",
      "    transform: 0.7110613631684687\n",
      "    transform: 0.7029191382059731\n",
      "    transform: -0.01721693796152869\n",
      "    transform: 1.494885147853866\n",
      "    transform: -0.7030535398885329\n",
      "    transform: 0.7111319055643803\n",
      "    transform: -0.0026707561806083883\n",
      "    transform: -0.09636892278268996\n",
      "    transform: 0.010366188267734094\n",
      "    transform: 0.014003500710368023\n",
      "    transform: 0.9998482105343054\n",
      "    transform: 2.115619227448896\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 1.0\n",
      "  }\n",
      "  width: 1920\n",
      "  height: 1280\n",
      "  rolling_shutter_direction: RIGHT_TO_LEFT\n",
      "}\n",
      "camera_calibrations {\n",
      "  name: SIDE_LEFT\n",
      "  intrinsic: 2076.1811337146355\n",
      "  intrinsic: 2076.1811337146355\n",
      "  intrinsic: 990.0281697291939\n",
      "  intrinsic: 241.46281117992226\n",
      "  intrinsic: 0.04928692476207616\n",
      "  intrinsic: -0.34392914437617744\n",
      "  intrinsic: 0.0023430477974171115\n",
      "  intrinsic: 0.0007565136814982747\n",
      "  intrinsic: 0.0\n",
      "  extrinsic {\n",
      "    transform: 0.0007773674626140717\n",
      "    transform: -0.9999544925788731\n",
      "    transform: 0.009508336929044875\n",
      "    transform: 1.4319986098354978\n",
      "    transform: 0.9999953948876524\n",
      "    transform: 0.000749435473798957\n",
      "    transform: -0.002940841709227941\n",
      "    transform: 0.11595612014948509\n",
      "    transform: 0.0029335819941143535\n",
      "    transform: 0.009510579256742532\n",
      "    transform: 0.9999504702628451\n",
      "    transform: 2.115959785688142\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 1.0\n",
      "  }\n",
      "  width: 1920\n",
      "  height: 886\n",
      "  rolling_shutter_direction: RIGHT_TO_LEFT\n",
      "}\n",
      "camera_calibrations {\n",
      "  name: SIDE_RIGHT\n",
      "  intrinsic: 2074.8857573225537\n",
      "  intrinsic: 2074.8857573225537\n",
      "  intrinsic: 1003.9971325425103\n",
      "  intrinsic: 238.44709517989804\n",
      "  intrinsic: 0.04289697738220451\n",
      "  intrinsic: -0.33138635154302365\n",
      "  intrinsic: 0.0012455007322764611\n",
      "  intrinsic: -8.338132004573235e-05\n",
      "  intrinsic: 0.0\n",
      "  extrinsic {\n",
      "    transform: 0.006786477634291073\n",
      "    transform: 0.9999769404419077\n",
      "    transform: 0.00024961121890264114\n",
      "    transform: 1.4298762217224124\n",
      "    transform: -0.9999673412652808\n",
      "    transform: 0.006787507568524859\n",
      "    transform: -0.004387042722913697\n",
      "    transform: -0.11562741288756147\n",
      "    transform: -0.004388635797684665\n",
      "    transform: -0.0002198304995963268\n",
      "    transform: 0.9999903457285908\n",
      "    transform: 2.1153736707023656\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 1.0\n",
      "  }\n",
      "  width: 1920\n",
      "  height: 886\n",
      "  rolling_shutter_direction: RIGHT_TO_LEFT\n",
      "}\n",
      "laser_calibrations {\n",
      "  name: FRONT\n",
      "  beam_inclination_min: -1.5707963267948966\n",
      "  beam_inclination_max: 0.5235987755982988\n",
      "  extrinsic {\n",
      "    transform: 0.9998668699072513\n",
      "    transform: 0.01068194351437736\n",
      "    transform: 0.01233444545294474\n",
      "    transform: 4.07\n",
      "    transform: -0.010715722036114829\n",
      "    transform: 0.9999390050606141\n",
      "    transform: 0.0026757166580387804\n",
      "    transform: 0.0\n",
      "    transform: -0.012305111259990328\n",
      "    transform: -0.002807532928575302\n",
      "    transform: 0.9999203478256327\n",
      "    transform: 0.691\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 1.0\n",
      "  }\n",
      "}\n",
      "laser_calibrations {\n",
      "  name: REAR\n",
      "  beam_inclination_min: -1.5707963267948966\n",
      "  beam_inclination_max: 0.5235987755982988\n",
      "  extrinsic {\n",
      "    transform: -0.9999954615415251\n",
      "    transform: -0.002970629151337338\n",
      "    transform: -0.0005022537181981321\n",
      "    transform: -1.154\n",
      "    transform: 0.0029694627223425013\n",
      "    transform: -0.9999929290711635\n",
      "    transform: 0.0023074008788306396\n",
      "    transform: 0.0\n",
      "    transform: -0.0005091045991123086\n",
      "    transform: 0.0023058989830942187\n",
      "    transform: 0.9999972118173064\n",
      "    transform: 0.466\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 1.0\n",
      "  }\n",
      "}\n",
      "laser_calibrations {\n",
      "  name: SIDE_LEFT\n",
      "  beam_inclination_min: -1.5707963267948966\n",
      "  beam_inclination_max: 0.5235987755982988\n",
      "  extrinsic {\n",
      "    transform: 0.026782591901166743\n",
      "    transform: -0.9995910662234105\n",
      "    transform: 0.010019635592216343\n",
      "    transform: 3.245\n",
      "    transform: 0.9996217498118772\n",
      "    transform: 0.02684338092960774\n",
      "    transform: 0.00598249139645594\n",
      "    transform: 1.025\n",
      "    transform: -0.006249005848633493\n",
      "    transform: 0.009855619037545145\n",
      "    transform: 0.9999319060312509\n",
      "    transform: 0.981\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 1.0\n",
      "  }\n",
      "}\n",
      "laser_calibrations {\n",
      "  name: SIDE_RIGHT\n",
      "  beam_inclination_min: -1.5707963267948966\n",
      "  beam_inclination_max: 0.5235987755982988\n",
      "  extrinsic {\n",
      "    transform: -0.007856674144848988\n",
      "    transform: 0.9999500763836013\n",
      "    transform: 0.006173930013477093\n",
      "    transform: 3.245\n",
      "    transform: -0.9996985303445468\n",
      "    transform: -0.007710771425717948\n",
      "    transform: -0.023310779287134407\n",
      "    transform: -1.025\n",
      "    transform: -0.023262009765599025\n",
      "    transform: -0.006355213957844648\n",
      "    transform: 0.999709202797101\n",
      "    transform: 0.981\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 1.0\n",
      "  }\n",
      "}\n",
      "laser_calibrations {\n",
      "  name: TOP\n",
      "  beam_inclinations: -0.3068221643606317\n",
      "  beam_inclinations: -0.29616997127840206\n",
      "  beam_inclinations: -0.2853962204525353\n",
      "  beam_inclinations: -0.2746601303299283\n",
      "  beam_inclinations: -0.2644333813317894\n",
      "  beam_inclinations: -0.2547203518161254\n",
      "  beam_inclinations: -0.2451132959715716\n",
      "  beam_inclinations: -0.23558114040681422\n",
      "  beam_inclinations: -0.22619708514421144\n",
      "  beam_inclinations: -0.21731641865081186\n",
      "  beam_inclinations: -0.20758272607836958\n",
      "  beam_inclinations: -0.19887296032455892\n",
      "  beam_inclinations: -0.19035328460083245\n",
      "  beam_inclinations: -0.18198429312782394\n",
      "  beam_inclinations: -0.1736864756471841\n",
      "  beam_inclinations: -0.16585362025780892\n",
      "  beam_inclinations: -0.15781802751504315\n",
      "  beam_inclinations: -0.15015955231667588\n",
      "  beam_inclinations: -0.14255145051670604\n",
      "  beam_inclinations: -0.135341813293673\n",
      "  beam_inclinations: -0.12822897363273378\n",
      "  beam_inclinations: -0.12122709544366828\n",
      "  beam_inclinations: -0.11464284303455963\n",
      "  beam_inclinations: -0.10786407088929861\n",
      "  beam_inclinations: -0.10122563229416759\n",
      "  beam_inclinations: -0.0949103749691842\n",
      "  beam_inclinations: -0.08921964911100977\n",
      "  beam_inclinations: -0.08333436696230079\n",
      "  beam_inclinations: -0.0779083231387212\n",
      "  beam_inclinations: -0.0725217846742372\n",
      "  beam_inclinations: -0.06735713829370016\n",
      "  beam_inclinations: -0.06240423882729429\n",
      "  beam_inclinations: -0.057603335746683415\n",
      "  beam_inclinations: -0.052982910920792214\n",
      "  beam_inclinations: -0.048682467189922196\n",
      "  beam_inclinations: -0.044331819106485204\n",
      "  beam_inclinations: -0.040285724475958684\n",
      "  beam_inclinations: -0.036195266813787486\n",
      "  beam_inclinations: -0.03242534844151601\n",
      "  beam_inclinations: -0.028847612027413705\n",
      "  beam_inclinations: -0.025949854706149633\n",
      "  beam_inclinations: -0.022544743478313256\n",
      "  beam_inclinations: -0.019545621162384785\n",
      "  beam_inclinations: -0.01633934133029724\n",
      "  beam_inclinations: -0.013559296624130246\n",
      "  beam_inclinations: -0.010780719883858314\n",
      "  beam_inclinations: -0.008031332258054524\n",
      "  beam_inclinations: -0.005073299090986749\n",
      "  beam_inclinations: -0.001984334363384077\n",
      "  beam_inclinations: 0.0007478706918118139\n",
      "  beam_inclinations: 0.003908920510252933\n",
      "  beam_inclinations: 0.006693257908674477\n",
      "  beam_inclinations: 0.009380296925375564\n",
      "  beam_inclinations: 0.012285848006322597\n",
      "  beam_inclinations: 0.015325464639903252\n",
      "  beam_inclinations: 0.01826429352258585\n",
      "  beam_inclinations: 0.021376977526962104\n",
      "  beam_inclinations: 0.02423980582923657\n",
      "  beam_inclinations: 0.026783223797488986\n",
      "  beam_inclinations: 0.029773578747043672\n",
      "  beam_inclinations: 0.03278254592456431\n",
      "  beam_inclinations: 0.035864543330808685\n",
      "  beam_inclinations: 0.038801578650298874\n",
      "  beam_inclinations: 0.04173696755489331\n",
      "  beam_inclination_min: -0.3121482609017465\n",
      "  beam_inclination_max: 0.04320466200719053\n",
      "  extrinsic {\n",
      "    transform: -0.8526719509207284\n",
      "    transform: -0.5224378704141576\n",
      "    transform: -0.0030357322277815815\n",
      "    transform: 1.43\n",
      "    transform: 0.5224451202853144\n",
      "    transform: -0.8526692389088371\n",
      "    transform: -0.0025030598650299957\n",
      "    transform: 0.0\n",
      "    transform: -0.0012807822227881296\n",
      "    transform: -0.0037202924272838555\n",
      "    transform: 0.9999922594806188\n",
      "    transform: 2.184\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 0.0\n",
      "    transform: 1.0\n",
      "  }\n",
      "}\n",
      "stats {\n",
      "  laser_object_counts {\n",
      "    type: TYPE_VEHICLE\n",
      "    count: 20\n",
      "  }\n",
      "  laser_object_counts {\n",
      "    type: TYPE_SIGN\n",
      "    count: 5\n",
      "  }\n",
      "  time_of_day: \"Day\"\n",
      "  location: \"location_phx\"\n",
      "  weather: \"sunny\"\n",
      "  camera_object_counts {\n",
      "    type: TYPE_VEHICLE\n",
      "    count: 31\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(frame.context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the *.context* command we have printed a number of data assocaited with each frame. Much of it consists of sensor callibration information, but as we can see there is also information on the likes of weather conditions and object detections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, I was not able to write up my investigations into the remaining topics as much as I would have liked due to time contraints, but I have tried to give them a quick reflection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voxelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voxels are essentially the 3D equivalent of pixels, and the process of voxelization takes a 3D point cloud and constructs a regular grid of voxels that may or may not contain points from the cloud. It appears there are some third party tools to complete this process, and at the very least there is research on this technique specifically for deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Sparse Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given something like a voxel grid where many of the voxels are empty and thus the grid is very sparse, it is very inefficient to perform regular 3D convolutions. Thus sparse 3D convolutions only examine active sites and are very efficient when dealing with sparse data. A PyTorch FacebookResearch implementation of this is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ego Motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This term is essentially equivalent to Odometry which is the process of determining a change in position over time. Becuase the LiDAR sensor is moving the frame of reference is too, so this needs to be accounted for when using multiple frames. All this boils down to is essentially a coordinate transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph convolutions are a type of convolution an arbitrary graph data structures that use the relations of nodes to propogate features. More investigation is needed on my part to full understand this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
